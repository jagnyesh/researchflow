"""
ResearchFlow Admin Dashboard

Streamlit interface for administrators to monitor system and review escalations.
"""

import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import asyncio
import time
import sys
import os
from dotenv import load_dotenv
from sqlalchemy import select

# Load environment variables
load_dotenv()

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.orchestrator import ResearchRequestOrchestrator
from app.agents import (
    RequirementsAgent,
    PhenotypeValidationAgent,
    CalendarAgent,
    DataExtractionAgent,
    QualityAssuranceAgent,
    DeliveryAgent
)
from app.database import get_db_session
from app.database.models import AgentExecution
from app.services.approval_service import ApprovalService


def initialize_orchestrator():
    """Initialize orchestrator with all agents"""
    if 'orchestrator' not in st.session_state:
        orchestrator = ResearchRequestOrchestrator()

        # Get HAPI FHIR database URL from environment
        hapi_db_url = os.getenv("HAPI_DB_URL", "postgresql://hapi:hapi@localhost:5433/hapi")

        # Register all agents (phenotype agent needs HAPI database for ViewDefinitions)
        orchestrator.register_agent('requirements_agent', RequirementsAgent())
        orchestrator.register_agent('phenotype_agent', PhenotypeValidationAgent(database_url=hapi_db_url))
        orchestrator.register_agent('calendar_agent', CalendarAgent())
        orchestrator.register_agent('extraction_agent', DataExtractionAgent())
        orchestrator.register_agent('qa_agent', QualityAssuranceAgent())
        orchestrator.register_agent('delivery_agent', DeliveryAgent())

        st.session_state.orchestrator = orchestrator


def main():
    """Main admin dashboard application"""
    st.set_page_config(
        page_title="ResearchFlow - Admin Dashboard",
        page_icon="âš™ï¸",
        layout="wide"
    )

    # Initialize orchestrator
    initialize_orchestrator()

    # Header
    st.title("âš™ï¸ ResearchFlow - Admin Dashboard")
    st.caption("System monitoring and management")

    # Auto-refresh controls
    col1, col2, col3, col4 = st.columns([2, 2, 2, 6])

    with col1:
        auto_refresh = st.checkbox("Auto-refresh", value=False, key="auto_refresh")

    with col2:
        refresh_interval = st.selectbox(
            "Interval",
            options=[5, 10, 30, 60],
            format_func=lambda x: f"{x}s",
            key="refresh_interval",
            disabled=not auto_refresh
        )

    with col3:
        if st.button("ðŸ”„ Refresh Now", key="manual_refresh"):
            st.rerun()

    with col4:
        if 'last_refresh' in st.session_state:
            st.caption(f"Last updated: {st.session_state.last_refresh.strftime('%H:%M:%S')}")

    # Update last refresh time
    st.session_state.last_refresh = datetime.now()

    # Tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ðŸ“Š Overview",
        "ðŸ¤– Agent Metrics",
        "âœ‹ Pending Approvals",
        "ðŸš¨ Escalations",
        "ðŸ“ˆ Analytics"
    ])

    with tab1:
        show_overview()

    with tab2:
        show_agent_metrics()

    with tab3:
        show_pending_approvals()

    with tab4:
        show_escalations()

    with tab5:
        show_analytics()

    # Auto-refresh logic
    if auto_refresh:
        time.sleep(refresh_interval)
        st.rerun()


def show_overview():
    """Display system overview"""
    st.header("System Overview")

    # Get all active requests
    requests = asyncio.run(st.session_state.orchestrator.get_all_active_requests())

    # Metrics
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Total Requests", len(requests))

    with col2:
        in_progress = len([r for r in requests if r['current_state'] not in ['delivered', 'complete', 'failed']])
        st.metric("In Progress", in_progress)

    with col3:
        completed = len([r for r in requests if r['current_state'] in ['delivered', 'complete']])
        st.metric("Completed", completed)

    with col4:
        failed = len([r for r in requests if r['current_state'] == 'failed'])
        st.metric("Failed/Escalated", failed, delta=-failed if failed > 0 else None)

    # Recent requests
    st.subheader("ðŸ“‹ Recent Requests")

    if requests:
        df_data = []
        for req in requests[:10]:  # Show last 10
            df_data.append({
                "Request ID": req['request_id'][:16] + "...",
                "Researcher": req['researcher_info'].get('name', 'N/A'),
                "Status": req['current_state'],
                "Current Agent": req.get('current_agent', 'None'),
                "Started": req['started_at'][:19]
            })

        df = pd.DataFrame(df_data)
        st.dataframe(df, use_container_width=True, hide_index=True)
    else:
        st.info("No requests yet")


def show_agent_metrics():
    """
    Display agent performance metrics from database

    NOTE: Queries AgentExecution table instead of in-memory state
    to support cross-process visibility (Researcher Portal â†’ Admin Dashboard)
    """
    st.header("ðŸ¤– Agent Performance Metrics")

    # Query AgentExecution table from database
    async def fetch_agent_metrics():
        async with get_db_session() as session:
            # Get all agent executions
            result = await session.execute(
                select(AgentExecution).order_by(
                    AgentExecution.started_at.desc()
                )
            )
            all_executions = result.scalars().all()

            # Calculate metrics per agent
            metrics_by_agent = {}
            for execution in all_executions:
                agent_id = execution.agent_id
                if agent_id not in metrics_by_agent:
                    metrics_by_agent[agent_id] = {
                        'total_tasks': 0,
                        'successful_tasks': 0,
                        'failed_tasks': 0,
                        'durations': [],
                        'state': 'idle'
                    }

                metrics_by_agent[agent_id]['total_tasks'] += 1

                if execution.status == 'success':
                    metrics_by_agent[agent_id]['successful_tasks'] += 1
                elif execution.status == 'failed':
                    metrics_by_agent[agent_id]['failed_tasks'] += 1

                if execution.completed_at and execution.started_at:
                    duration = (
                        execution.completed_at - execution.started_at
                    ).total_seconds()
                    metrics_by_agent[agent_id]['durations'].append(duration)

            # Calculate success rate and avg duration
            for agent_id, metrics in metrics_by_agent.items():
                metrics['success_rate'] = (
                    metrics['successful_tasks'] / metrics['total_tasks']
                    if metrics['total_tasks'] > 0 else 0
                )
                metrics['avg_duration_seconds'] = (
                    sum(metrics['durations']) / len(metrics['durations'])
                    if metrics['durations'] else 0
                )

            return metrics_by_agent

    all_metrics = asyncio.run(fetch_agent_metrics())

    if not all_metrics:
        st.info("No agent metrics available yet")
        return

    # Create metrics table
    metrics_data = []
    for agent_id, metrics in all_metrics.items():
        agent_name = agent_id.replace('_', ' ').title()
        metrics_data.append({
            "Agent": agent_name,
            "State": metrics['state'],
            "Total Tasks": metrics['total_tasks'],
            "Successful": metrics['successful_tasks'],
            "Failed": metrics['failed_tasks'],
            "Success Rate": f"{metrics['success_rate']:.1%}",
            "Avg Duration (s)": f"{metrics['avg_duration_seconds']:.1f}"
        })

    df = pd.DataFrame(metrics_data)
    st.dataframe(df, use_container_width=True, hide_index=True)

    # Agent status indicators
    st.subheader("Agent Status")

    cols = st.columns(3)
    for idx, (agent_id, metrics) in enumerate(all_metrics.items()):
        with cols[idx % 3]:
            agent_name = agent_id.replace('_', ' ').title()
            status = metrics['state']
            status_emoji = (
                "ðŸŸ¢" if status == "idle"
                else "ðŸ”µ" if status == "working"
                else "ðŸ”´"
            )

            st.metric(
                f"{status_emoji} {agent_name}",
                status,
                f"{metrics['total_tasks']} tasks"
            )


def show_pending_approvals():
    """Display pending approvals requiring human review"""
    st.header("âœ‹ Pending Approvals - Human-in-Loop Gates")

    # Filter by approval type
    col1, col2, col3 = st.columns([2, 2, 6])

    with col1:
        approval_type_filter = st.selectbox(
            "Filter by Type",
            ["All", "Requirements", "Phenotype SQL", "Extraction", "QA", "Scope Change"],
            key="approval_type_filter"
        )

    with col2:
        reviewer_email = st.text_input(
            "Your Email",
            placeholder="informatician@hospital.org",
            key="reviewer_email"
        )

    # Fetch pending approvals from database
    try:
        # Map UI selections to database values
        type_map = {
            "All": None,
            "Requirements": "requirements",
            "Phenotype SQL": "phenotype_sql",
            "Extraction": "extraction",
            "QA": "qa",
            "Scope Change": "scope_change"
        }

        approval_type_param = type_map[approval_type_filter]

        # Fetch approvals directly from database
        async def fetch_approvals():
            async with get_db_session() as session:
                approval_service = ApprovalService(session)
                approvals = await approval_service.get_pending_approvals(
                    approval_type=approval_type_param
                )
                return approvals

        approvals_db = asyncio.run(fetch_approvals())

        # Convert to dict format for display
        approvals = [
            {
                "id": approval.id,
                "request_id": approval.request_id,
                "approval_type": approval.approval_type,
                "submitted_at": approval.submitted_at.isoformat(),
                "submitted_by": approval.submitted_by,
                "timeout_at": approval.timeout_at.isoformat() if approval.timeout_at else None,
                "approval_data": approval.approval_data
            }
            for approval in approvals_db
        ]

        # Metrics
        st.subheader("ðŸ“Š Approval Queue Metrics")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("Total Pending", len(approvals))

        with col2:
            sql_approvals = len([a for a in approvals if a.get('approval_type') == 'phenotype_sql'])
            st.metric("ðŸ”´ SQL Reviews", sql_approvals, help="CRITICAL - SQL must be approved before execution")

        with col3:
            req_approvals = len([a for a in approvals if a.get('approval_type') == 'requirements'])
            st.metric("Requirements", req_approvals)

        with col4:
            scope_approvals = len([a for a in approvals if a.get('approval_type') == 'scope_change'])
            st.metric("Scope Changes", scope_approvals)

        # Display approvals
        st.subheader("ðŸ“‹ Pending Approvals")

        if not approvals:
            st.success("âœ… No pending approvals - all reviews complete!")
        else:
            for approval in approvals:
                display_approval_card(approval, reviewer_email)

    except Exception as e:
        st.error(f"Error fetching approvals: {str(e)}")
        import traceback
        st.code(traceback.format_exc())


def display_approval_card(approval, reviewer_email):
    """Display a single approval card with all details and action buttons"""
    approval_id = approval['id']
    approval_type = approval['approval_type']
    request_id = approval['request_id']
    submitted_at = approval['submitted_at']
    submitted_by = approval.get('submitted_by', 'Unknown')
    timeout_at = approval.get('timeout_at')
    approval_data = approval.get('approval_data', {})

    # Determine urgency based on timeout
    urgency = "ðŸŸ¢ Normal"
    if timeout_at:
        timeout_dt = datetime.fromisoformat(timeout_at.replace('Z', '+00:00'))
        time_remaining = timeout_dt - datetime.now()
        hours_remaining = time_remaining.total_seconds() / 3600

        if hours_remaining < 2:
            urgency = "ðŸ”´ URGENT (< 2h)"
        elif hours_remaining < 6:
            urgency = "ðŸŸ¡ High Priority (< 6h)"

    # Type-specific styling
    type_emoji_map = {
        "requirements": "ðŸ“„",
        "phenotype_sql": "ðŸ”´",  # Red circle for critical SQL
        "extraction": "ðŸ“Š",
        "qa": "âœ“",
        "scope_change": "ðŸ”„"
    }

    type_label_map = {
        "requirements": "Requirements Review",
        "phenotype_sql": "SQL REVIEW (CRITICAL)",
        "extraction": "Extraction Approval",
        "qa": "QA Review",
        "scope_change": "Scope Change"
    }

    emoji = type_emoji_map.get(approval_type, "ðŸ“‹")
    label = type_label_map.get(approval_type, approval_type.title())

    # Create expander with urgency indicator
    with st.expander(f"{emoji} {label} - {request_id[:20]}... | {urgency}", expanded=(approval_type == "phenotype_sql")):
        # Header info
        col1, col2, col3 = st.columns(3)
        with col1:
            st.write(f"**Request ID:** `{request_id}`")
        with col2:
            st.write(f"**Submitted:** {submitted_at[:19]}")
        with col3:
            if timeout_at:
                st.write(f"**Timeout:** {timeout_at[:19]}")

        st.write(f"**Submitted by:** {submitted_by}")

        st.divider()

        # Display type-specific approval data
        if approval_type == "phenotype_sql":
            display_sql_approval(approval_data)

        elif approval_type == "requirements":
            display_requirements_approval(approval_data)

        elif approval_type == "scope_change":
            display_scope_change_approval(approval_data)

        elif approval_type == "qa":
            display_qa_approval(approval_data)

        elif approval_type == "extraction":
            display_extraction_approval(approval_data)

        st.divider()

        # Action buttons
        st.subheader("Actions")

        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("âœ… Approve", key=f"approve_{approval_id}", type="primary", use_container_width=True):
                if not reviewer_email:
                    st.error("Please enter your email address")
                else:
                    handle_approval_response(approval_id, "approve", reviewer_email, "", {})

        with col2:
            if st.button("âœï¸ Modify & Approve", key=f"modify_{approval_id}", use_container_width=True):
                st.session_state[f'show_modify_{approval_id}'] = True

        with col3:
            if st.button("âŒ Reject", key=f"reject_{approval_id}", use_container_width=True):
                st.session_state[f'show_reject_{approval_id}'] = True

        # Modification interface
        if st.session_state.get(f'show_modify_{approval_id}', False):
            st.subheader("Approve with Modifications")
            notes = st.text_area("Notes", placeholder="Explain your modifications...", key=f"modify_notes_{approval_id}")

            if approval_type == "phenotype_sql":
                modified_sql = st.text_area(
                    "Modified SQL Query",
                    value=approval_data.get('sql_query', ''),
                    height=200,
                    key=f"modified_sql_{approval_id}"
                )
                modifications = {"sql_query": modified_sql}
            else:
                modifications = {}

            if st.button("Submit Modifications", key=f"submit_modify_{approval_id}"):
                if not reviewer_email:
                    st.error("Please enter your email address")
                else:
                    handle_approval_response(approval_id, "modify", reviewer_email, notes, modifications)
                    st.session_state[f'show_modify_{approval_id}'] = False

        # Rejection interface
        if st.session_state.get(f'show_reject_{approval_id}', False):
            st.subheader("Reject Approval")
            reject_reason = st.text_area(
                "Reason for Rejection",
                placeholder="Explain why this is being rejected and what needs to be fixed...",
                key=f"reject_reason_{approval_id}"
            )

            if st.button("Confirm Rejection", key=f"confirm_reject_{approval_id}", type="secondary"):
                if not reviewer_email:
                    st.error("Please enter your email address")
                elif not reject_reason:
                    st.error("Please provide a reason for rejection")
                else:
                    handle_approval_response(approval_id, "reject", reviewer_email, reject_reason, {})
                    st.session_state[f'show_reject_{approval_id}'] = False


def display_sql_approval(data):
    """Display SQL approval details with syntax highlighting"""
    st.subheader("ðŸ”´ CRITICAL: SQL Query Review")

    st.warning("""
    **IMPORTANT:** This SQL query will execute against the production FHIR database if approved.
    Please verify:
    - SQL syntax is correct
    - Date filters are appropriate
    - Cohort size is reasonable
    - No sensitive fields are exposed
    - Joins and selections are accurate
    """)

    # SQL Query
    sql_query = data.get('sql_query', 'N/A')
    st.code(sql_query, language='sql')

    # Metrics
    col1, col2, col3 = st.columns(3)

    with col1:
        cohort = data.get('estimated_cohort', 'Unknown')
        st.metric("Estimated Cohort", cohort)

    with col2:
        feasibility = data.get('feasibility_score', 0)
        st.metric("Feasibility Score", f"{feasibility:.2f}")

    with col3:
        availability = data.get('data_availability', {}).get('overall_availability', 0)
        st.metric("Data Availability", f"{availability:.1%}")

    # Warnings
    warnings = data.get('warnings', [])
    if warnings:
        st.warning("âš ï¸ **Warnings:**")
        for warning in warnings:
            st.write(f"- {warning.get('message', warning)}")

    # Recommendations
    recommendations = data.get('recommendations', [])
    if recommendations:
        st.info("ðŸ’¡ **Recommendations:**")
        for rec in recommendations:
            st.write(f"- {rec}")


def display_requirements_approval(data):
    """Display requirements approval details"""
    st.subheader("ðŸ“„ Requirements Review")

    structured_reqs = data.get('structured_requirements', {})
    completeness = data.get('completeness_score', 0)

    # Completeness score
    st.metric("Completeness Score", f"{completeness:.1%}")

    # Study details
    if 'study_title' in structured_reqs:
        st.write(f"**Study Title:** {structured_reqs['study_title']}")

    # Inclusion criteria
    inclusion = structured_reqs.get('inclusion_criteria', [])
    if inclusion:
        st.write("**Inclusion Criteria:**")
        for criterion in inclusion:
            st.write(f"- {criterion}")

    # Exclusion criteria
    exclusion = structured_reqs.get('exclusion_criteria', [])
    if exclusion:
        st.write("**Exclusion Criteria:**")
        for criterion in exclusion:
            st.write(f"- {criterion}")

    # Data elements
    elements = structured_reqs.get('data_elements', [])
    if elements:
        st.write(f"**Data Elements:** {', '.join(elements)}")

    # Full requirements (using details/summary instead of nested expander)
    st.markdown("---")
    st.markdown("**Full Requirements (JSON):**")
    st.json(structured_reqs)


def display_scope_change_approval(data):
    """Display scope change approval with impact analysis"""
    st.subheader("ðŸ”„ Scope Change Request")

    requested_changes = data.get('requested_changes', {})
    reason = data.get('reason', 'N/A')
    impact = data.get('impact_analysis', {})

    # Reason
    st.write(f"**Reason:** {reason}")

    # Requested changes
    st.write("**Requested Changes:**")
    st.json(requested_changes)

    st.divider()

    # Impact analysis
    st.subheader("ðŸ“Š Impact Analysis")

    severity = impact.get('severity', 'unknown')
    severity_color = {"low": "ðŸŸ¢", "medium": "ðŸŸ¡", "high": "ðŸ”´"}.get(severity, "âšª")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("Severity", f"{severity_color} {severity.title()}")

    with col2:
        requires_rework = impact.get('requires_rework', False)
        st.metric("Requires Rework", "Yes" if requires_rework else "No")

    with col3:
        delay = impact.get('estimated_delay_hours', 0)
        st.metric("Estimated Delay", f"{delay}h")

    # Restart point
    if impact.get('restart_from_state'):
        st.warning(f"âš ï¸ **Workflow will restart from:** `{impact['restart_from_state']}`")

    # Affected components
    affected = impact.get('affected_components', [])
    if affected:
        st.write(f"**Affected Components:** {', '.join(affected)}")


def display_qa_approval(data):
    """Display QA approval details"""
    st.subheader("âœ“ QA Results Review")

    st.write("**QA validation results:**")
    st.json(data)


def display_extraction_approval(data):
    """Display extraction approval details"""
    st.subheader("ðŸ“Š Data Extraction Approval")

    st.write("**Extraction details:**")
    st.json(data)


def handle_approval_response(approval_id, decision, reviewer, notes, modifications):
    """Handle approval response (approve/reject/modify) using direct database access"""
    try:
        async def process_approval():
            async with get_db_session() as session:
                approval_service = ApprovalService(session)

                if decision == "approve":
                    await approval_service.approve(
                        approval_id,
                        reviewer,
                        notes,
                        modifications
                    )
                elif decision == "reject":
                    await approval_service.reject(
                        approval_id,
                        reviewer,
                        notes or "Rejected"
                    )
                elif decision == "modify":
                    await approval_service.modify(
                        approval_id,
                        reviewer,
                        modifications,
                        notes
                    )
                else:
                    raise ValueError(f"Invalid decision: {decision}")

        asyncio.run(process_approval())

        if decision == "approve":
            st.success(f"âœ… Approval {approval_id} approved! Workflow will continue automatically.")
        elif decision == "reject":
            st.error(f"âŒ Approval {approval_id} rejected. Request will return to originating agent.")
        elif decision == "modify":
            st.success(f"âœï¸ Approval {approval_id} approved with modifications. Workflow will continue with your changes.")

        # Trigger refresh after 2 seconds
        time.sleep(2)
        st.rerun()

    except Exception as e:
        st.error(f"Error processing approval: {str(e)}")
        import traceback
        st.code(traceback.format_exc())


def show_escalations():
    """Display escalations requiring human review"""
    st.header("ðŸš¨ Escalations & Human Review Queue")

    st.info("""
    **Note:** Escalations will appear here when:
    - Agents encounter errors after max retries
    - Data quality checks fail critically
    - Requests are not feasible
    - Complex decisions require human judgment
    """)

    # Mock escalation data for demonstration
    st.subheader("Pending Reviews")

    escalations = []  # TODO: Retrieve from database

    if escalations:
        for escalation in escalations:
            with st.expander(f"Request {escalation['request_id']} - {escalation['reason']}"):
                st.write(f"**Agent:** {escalation['agent']}")
                st.write(f"**Error:** {escalation['error']}")

                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("âœ… Approve", key=f"approve_{escalation['id']}"):
                        st.success("Approved")
                with col2:
                    if st.button("âœï¸ Modify", key=f"modify_{escalation['id']}"):
                        st.info("Show modification interface")
                with col3:
                    if st.button("âŒ Reject", key=f"reject_{escalation['id']}"):
                        st.error("Rejected")
    else:
        st.success("No pending escalations - all systems nominal!")


def show_analytics():
    """Display analytics and insights"""
    st.header("ðŸ“ˆ Analytics & Insights")

    # Request volume over time
    st.subheader("Request Volume Trends")

    # Mock data for demonstration
    dates = pd.date_range(start=datetime.now() - timedelta(days=30), end=datetime.now(), freq='D')
    volume_data = pd.DataFrame({
        'Date': dates,
        'Submitted': [i % 5 + 2 for i in range(len(dates))],
        'Completed': [i % 4 + 1 for i in range(len(dates))]
    })

    st.line_chart(volume_data.set_index('Date'))

    # ROI Metrics
    st.subheader("ðŸ’° ROI Analysis")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Manual Cost/Request", "$1,500")

    with col2:
        st.metric("Automated Cost/Request", "$150", delta="-$1,350")

    with col3:
        st.metric("Time Saved (hours)", "1,200", delta="+150 this month")

    with col4:
        st.metric("Total Savings", "$75,000", delta="+$7,500")

    # Most requested data elements
    st.subheader("Most Requested Data Elements")

    element_data = pd.DataFrame({
        'Element': ['Clinical Notes', 'Lab Results', 'Medications', 'Diagnoses', 'Procedures'],
        'Requests': [45, 38, 32, 28, 15]
    })

    st.bar_chart(element_data.set_index('Element'))


if __name__ == "__main__":
    main()
